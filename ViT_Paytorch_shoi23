# ----------------------------
# モデルのカスタム
# ----------------------------
print("=======モデルのカスタム=======")

# Get cpu, gpu or mps device for training.

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ----------------------------
# Input Layer
# ----------------------------
print("=======Input Layer=======")

class VitInputLayer(nn.Module): 
    def __init__(self, in_channels:int=3, emb_dim:int=384, num_patch_row:int=2, image_size:int=32):

        super(VitInputLayer, self).__init__() 
        self.in_channels=in_channels 
        self.emb_dim = emb_dim 
        self.num_patch_row = num_patch_row 
        self.image_size = image_size
     
        self.patch_size = int(self.image_size // self.num_patch_row)

        self.patch_emb_layer = nn.Conv2d(
            in_channels=self.in_channels, 
            out_channels=self.emb_dim, 
            kernel_size=self.patch_size, 
            stride=self.patch_size
        )

        self.cls_token = nn.Parameter(
            torch.randn(1, 1, emb_dim) 
        )

        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
       
        input_device = x.device

        z_0 = self.patch_emb_layer(x)

        print(z_0.shape)
        print(z_0.shape[0], z_0.shape[1], z_0.shape[2] * z_0.shape[3])
        
        z_0 = torch.reshape(z_0, (z_0.shape[0], z_0.shape[1], z_0.shape[2] * z_0.shape[3]))  #←z_0.flatten(2)

        z_0 = z_0.transpose(1, 2)

        print("***")
        print(z_0.shape)
        print(x.size)
        print(self.cls_token.shape)
        print(self.cls_token.repeat(repeats=(x.size(0),x.size(1),1)).shape)
        print("***")

        z_0 = torch.cat(
            [self.cls_token.repeat(repeats=(x.size(0),x.size(1),1)), z_0], dim=1)

        z_0 = z_0 + nn.Parameter(torch.randn(z_0.shape, device=input_device)) 
        return z_0                                    

batch_size, channel, height, width= 2, 3, 64, 64
x = torch.randn(batch_size, channel, height, width) 
input_layer = VitInputLayer(num_patch_row=2) 
z_0=input_layer(x)

print(z_0.shape)

# ----------------------------
# Self-Attention
# ----------------------------
print("=======Self-Attention=======")

class MultiHeadSelfAttention(nn.Module): 
    def __init__(self, emb_dim:int=384, head:int=3, dropout:float=0.):

        super(MultiHeadSelfAttention, self).__init__() 
        self.head = head
        self.emb_dim = emb_dim
        self.head_dim = emb_dim // head
        self.sqrt_dh = self.head_dim**0.5 

        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False) 
        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False) 
        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)

        self.attn_drop = nn.Dropout(dropout)

        self.w_o = nn.Sequential(
            nn.Linear(emb_dim, emb_dim),
            nn.Dropout(dropout) 
        )

    def forward(self, z: torch.Tensor) -> torch.Tensor:

        batch_size, num_patch, _ = z.size()

        q = self.w_q(z)
        k = self.w_k(z)
        v = self.w_v(z)

        q = q.view(batch_size, num_patch, self.head, self.head_dim)
        k = k.view(batch_size, num_patch, self.head, self.head_dim)
        v = v.view(batch_size, num_patch, self.head, self.head_dim)

        q = q.transpose(1,2)
        k = k.transpose(1,2)
        v = v.transpose(1,2)

      
        k_T = k.transpose(2, 3)

        dots = (q @ k_T) / self.sqrt_dh
        
        attn = F.softmax(dots, dim=-1)
     
        attn = self.attn_drop(attn)
       
        out = attn @ v

        out = out.transpose(1, 2)
       
        out = out.reshape(batch_size, num_patch, self.emb_dim)

        out = self.w_o(out) 
        return out

mhsa = MultiHeadSelfAttention()
out = mhsa(z_0) 
print(out.shape)


# ----------------------------
# Encoder
# ----------------------------
print("=======Encoder=======")

class VitEncoderBlock(nn.Module): 
    def __init__(self, emb_dim:int=384, head:int=8, hidden_dim:int=384*4, dropout: float=0.):
       
        super(VitEncoderBlock, self).__init__()
        # 1つ目のLayer Normalization
        self.ln1 = nn.LayerNorm(emb_dim)
        # MHSA [2-4-7項]
        self.msa = MultiHeadSelfAttention(
        emb_dim=emb_dim, head=head,
        dropout = dropout,
        )

        self.ln2 = nn.LayerNorm(emb_dim)

        self.mlp = nn.Sequential( 
            nn.Linear(emb_dim, hidden_dim), 
            nn.GELU(),
            nn.Dropout(dropout), 
            nn.Linear(hidden_dim, emb_dim), 
            nn.Dropout(dropout)
        )
    def forward(self, z: torch.Tensor) -> torch.Tensor:
       
        out = self.msa(self.ln1(z)) + z

        out = self.mlp(self.ln2(out)) + out 
        return out

vit_enc = VitEncoderBlock()
z_1 = vit_enc(z_0) #z_0は2-2節のz_0=input_layer(x)で、形状は(B, N, D)

# (2, 5, 384)(=(B, N, D))になっていることを確認 
print(z_1.shape)


# ----------------------------
# ViTの実装
# ----------------------------
print("=======ViTの実装=======")

class Vit(nn.Module): 
    def __init__(self, in_channels:int=3, num_classes:int=500, emb_dim:int=384,
                 num_patch_row:int=2, image_size:int=64, num_blocks:int=7, head:int=8, hidden_dim:int=384*4, dropout:float=0.):
       
        super(Vit, self).__init__()
        # Input Layer 
        self.input_layer = VitInputLayer(
            in_channels, 
            emb_dim, 
            num_patch_row, 
            image_size)

        self.encoder = nn.Sequential(*[
            VitEncoderBlock(
                emb_dim=emb_dim,
                head=head,
                hidden_dim=hidden_dim,
                dropout = dropout
            )
            for _ in range(num_blocks)])

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(emb_dim),
            nn.Linear(emb_dim, num_classes)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        
        out = self.input_layer(x)
        
        out = self.encoder(out)

        cls_token = out[:,0]

        pred = self.mlp_head(cls_token)
        return pred

num_classes = 10
batch_size, channel, height, width= 2, 3, 64, 64
x = torch.randn(batch_size, channel, height, width)
vit = Vit(in_channels=channel, num_classes=num_classes) 
pred = vit(x)

print(pred.shape)

# ----------------------------
# 最適手法の定義
# ----------------------------
#GPUを使用させるために指定(PyTroch特有)
#model = VitInputLayer().to(device) #ここは不要(というより無意味なコードなので)コメントアウトしました
#model = MultiHeadSelfAttention().to(device)
#model = VitEncoderBlock().to(device)
model = Vit().to(device)

#最適化手法の定義
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


#Model Summary
from torchsummary import summary
summary(Vit(),(channel,height,width)) # summary(model,(channels,H,W))
